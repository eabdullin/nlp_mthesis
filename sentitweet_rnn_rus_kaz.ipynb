{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "np.random.seed(241)  # for reproducibility\n",
    "from gensim.models import Word2Vec\n",
    "import sklearn.cross_validation as valid\n",
    "import sklearn.metrics as metrics\n",
    "from keras.constraints import unitnorm\n",
    "from keras.layers.core import Dense,Dropout,Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.models import Sequential\n",
    "import re, string\n",
    "import io, csv\n",
    "from sklearn.cross_validation import KFold\n",
    "import sklearn.cross_validation as valid\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(241)\n",
    "punc = r'[\\!#$%&()*+,-./:;<=>?\\[\\]^_`{|}~\\\"]'\n",
    "punc_regex = re.compile(punc)\n",
    "space_regex = re.compile('\\s+')\n",
    "article_regex = re.compile(r'^.*\\nTEXT\\:', re.M | re.DOTALL)\n",
    "digits_regex = re.compile('\\d+')\n",
    "someOneRegex = re.compile(r\"@\\S+\\s\")\n",
    "urlfinder = re.compile(\"https?:\\/\\/\\S+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_text(text,soup=True):\n",
    "    if soup:\n",
    "        soup = BeautifulSoup(text, 'html.parser')\n",
    "        text = soup.get_text()\n",
    "    text = digits_regex.sub(u'0', text.lower())\n",
    "    text = urlfinder.sub(u' ReplacedUrl ', text)\n",
    "    text = punc_regex.sub(u' ', text)\n",
    "    text = space_regex.sub(u' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_sentences(data, maxlen=None, value=0.):\n",
    "    lengths = [len(s) for s in data]\n",
    "    nb_samples = len(data)\n",
    "    if maxlen is None:\n",
    "        maxlen = np.max(lengths)\n",
    "    x = (np.ones((nb_samples, maxlen)) * value).astype(np.int)\n",
    "    for idx, s in enumerate(data):\n",
    "        if len(s) == 0:\n",
    "            continue  # empty list was found\n",
    "        trunc = s[-maxlen:]\n",
    "        x[idx, -len(trunc):] = trunc\n",
    "    return x, maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load_word2vec_format('data\\\\bilingual.bin', binary=True)\n",
    "# w2v_model = Word2Vec.load('data\\\\bilingual_mixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_kaz(file_name, X, y):\n",
    "    with open(file_name, 'r') as csv_file:\n",
    "        reader = csv.reader(csv_file, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            try:\n",
    "                rowText = row[1].decode('utf-8')\n",
    "                sentiment = int(row[0])\n",
    "                if(sentiment == -1):\n",
    "                    sentiment = 0                  \n",
    "                if len(rowText) > 10:\n",
    "                    rowText = someOneRegex.sub('@someone', rowText)\n",
    "                    rowText = process_text(rowText, soup=False)\n",
    "                    sent = []\n",
    "                    words = rowText.split()\n",
    "                    for word in words:\n",
    "                        if word in w2v_model.vocab:\n",
    "                            sent.append(w2v_model.vocab[word].index)\n",
    "                    X.append(sent)\n",
    "                    y.append(sentiment)\n",
    "            except:\n",
    "                v = 0\n",
    "def twitter_parse_rus(file_name,sentiment, X, y):\n",
    "    with open(file_name, 'r') as csv_file:\n",
    "        reader = csv.reader(csv_file, delimiter=';')\n",
    "        for row in reader:\n",
    "            rowText = row[3].decode('utf-8')\n",
    "            if len(rowText) > 10:\n",
    "                rowText = someOneRegex.sub('@someone', rowText)\n",
    "                rowText = process_text(rowText, soup=False)\n",
    "                sent = []\n",
    "                words = rowText.split()\n",
    "                for word in words:\n",
    "                    if word in w2v_model.vocab:\n",
    "                        sent.append(w2v_model.vocab[word].index)\n",
    "                X.append(sent)\n",
    "                y.append(sentiment)\n",
    "                \n",
    "def twitter_parse_en(file_name, X, y):\n",
    "    with open(file_name, 'r') as csv_file:\n",
    "        reader = csv.reader(csv_file, delimiter=',')\n",
    "        for row in reader:\n",
    "            try:\n",
    "                rowText = row[5].decode('utf-8')\n",
    "                sentiment = int(row[0])\n",
    "                if(sentiment == 2):\n",
    "                    continue\n",
    "                elif(sentiment == 4):\n",
    "                    sentiment = 1                    \n",
    "                if len(rowText) > 10:\n",
    "                    rowText = someOneRegex.sub('@someone', rowText)\n",
    "                    rowText = process_text(rowText, soup=False)\n",
    "                    sent = []\n",
    "                    words = rowText.split()\n",
    "                    for word in words:\n",
    "                        if word in w2v_model.vocab:\n",
    "                            sent.append(w2v_model.vocab[word].index)\n",
    "                    X.append(sent)\n",
    "                    y.append(sentiment)\n",
    "            except:\n",
    "                v = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "554\n",
      "(554L, 60L)\n"
     ]
    }
   ],
   "source": [
    "X_kaz = []\n",
    "y_kaz = []\n",
    "# twitter_parse('data\\\\negative.csv',0,X,y)\n",
    "# twitter_parse('data\\\\positive.csv',1,X,y)\n",
    "parse_kaz('data\\\\kaz_news_comments.csv',X_kaz,y_kaz)\n",
    "print len(X_kaz)\n",
    "X_kaz, maxlen = pad_sentences(X_kaz)\n",
    "y_kaz = np.array(y_kaz)\n",
    "print X_kaz.shape\n",
    "X_kaz_train, X_kaz_test, y_kaz_train, y_kaz_test = valid.train_test_split(X_kaz, y_kaz, test_size=0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000L, 60L)\n"
     ]
    }
   ],
   "source": [
    "X_rus = []\n",
    "y_rus = []\n",
    "twitter_parse_rus('data\\\\rus_negative_twitter.csv',0,X_rus,y_rus)\n",
    "twitter_parse_rus('data\\\\rus_positive_twitter.csv',1,X_rus,y_rus)\n",
    "X_rus, maxlen = pad_sentences(X_rus, maxlen)\n",
    "y_rus = np.array(y_rus)\n",
    "ss = np.random.choice(X_rus.shape[0], 20000)\n",
    "X_rus = X_rus[ss]\n",
    "y_rus = y_rus[ss]\n",
    "print X_rus.shape\n",
    "X_rus_train, X_rus_test, y_rus_train, y_rus_test = valid.train_test_split(X_rus, y_rus, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.vstack((X_kaz_train,X_rus_train))\n",
    "y_train = np.hstack((y_kaz_train,y_rus_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set parameters:\n",
    "word_vocub_len = w2v_model.syn0.shape[0]\n",
    "word_maxlen= maxlen\n",
    "word_embedding_dims = w2v_model.syn0.shape[1]\n",
    "hidden_dims = 250\n",
    "nb_epoch = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compile start\n",
      "model compilled\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=word_vocub_len,output_dim=word_embedding_dims, input_length=word_maxlen,weights=[w2v_model.syn0], W_constraint=unitnorm()))#, weights=[wordvectors.W], W_constraint=unitnorm()\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(LSTM(64,return_sequences=True))\n",
    "model.add(GRU(64, dropout_W=0.2, dropout_U=0.2))\n",
    "# model.add(Dense(128,activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "print('compile start')\n",
    "rmsprop = RMSprop(lr=0.001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=rmsprop, metrics=[\"accuracy\"])\n",
    "print('model compilled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15110L, 60L)\n",
      "Epoch 1/1\n",
      "15110/15110 [==============================] - 252s - loss: 0.6114 - acc: 0.6661   \n",
      "fitting ended\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "('fit start')\n",
    "# ss_kaz = np.random.choice(X.shape[0], 15000)\n",
    "# ss_rus = np.random.choice(X_rus.shape[0], 500)\n",
    "# X_train, X_test, y_train, y_test = valid.train_test_split(np.vstack((X[ss_kaz],X_rus[ss_rus])), np.hstack((y[ss_kaz],y_rus[ss_rus])), test_size=0.3)\n",
    "print(X_train.shape)\n",
    "# for i in xrange(N_epoch):\n",
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=1, verbose=1)\n",
    "print('fitting ended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444/444 [==============================] - 0s     \n",
      "validation on kazakh dataset: accuracy = 0.55405, roc_auc =0.63825\n"
     ]
    }
   ],
   "source": [
    "y_predscore = model.predict_proba(X_kaz_test, batch_size=batch_size, verbose=1)\n",
    "y_pred = np.round(y_predscore)\n",
    "auc = metrics.roc_auc_score(y_kaz_test, y_predscore)\n",
    "acc = metrics.accuracy_score(y_kaz_test,y_pred)\n",
    "print 'validation on kazakh dataset: accuracy = {:.5}, roc_auc ={:.5}'.format( acc, auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 3s     \n",
      "validation on russian dataset: accuracy = 0.724, roc_auc =0.7998\n"
     ]
    }
   ],
   "source": [
    "y_predscore = model.predict_proba(X_rus_test, batch_size=batch_size, verbose=1)\n",
    "y_pred = np.round(y_predscore)\n",
    "auc = metrics.roc_auc_score(y_rus_test, y_predscore)\n",
    "acc = metrics.accuracy_score(y_rus_test, y_pred)\n",
    "print 'validation on russian dataset: accuracy = {:.5}, roc_auc ={:.5}'.format(acc, auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
